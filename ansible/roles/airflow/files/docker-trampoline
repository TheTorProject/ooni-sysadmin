#!/bin/bash
#
# Ability to execute arbituary docker commands equals to root-shell, so this
# script does some whitelisting.  The script should be launched with:
#
#   sudo --non-interactive /foo/bar/trampoline.sh \
#         do-something-awesome.py \
#         "{{ ds }}" \
#         "{{ execution_date.isoformat() }}" \
#         "{{ (execution_date + dag.schedule_interval).isoformat() }}" \
#         other CLI arguments to script

# FIXME: 1000 = `id -u benchmark` at the datacollector dom0
uidno=1000

script="$1"
bucket="$2"
isofrom="$3" # execution_date
isotill="$4" # execution_date + schedule_interval
shift 4

(
shopt -s extglob
for var in "$bucket" "$isofrom" "$isotill"; do
    # NB: `.` (dot) delimits microseconds in isoformat() but they are not welcome here
    if [ -z "$var" -o -n "${var/+([-0-9T:])/}" ]; then
        echo "$0: bad arg <$var> -> <${var/+([-0-9T:])/}>" 1>&2
        exit 1
    fi
done
) || exit 1

docksafe="--rm --interactive --read-only"
docknet="--network none"

private=/data/ooni/private
public=/data/ooni/public
volargs=""
case "$script" in
    reports_raw_sensor)
    volargs="${volargs} --volume=$private/reports-raw/${bucket}:$private/reports-raw/${bucket}:ro"
    volargs="${volargs} --volume=$private/reports-raw-shals:$private/reports-raw-shals:ro"
    exec docker run $docksafe $docknet $volargs debian:jessie \
        /bin/bash -c "set -o pipefail && find $private/reports-raw/${bucket} -type f -printf '%f %s\n' | LC_ALL=C sort | sha256sum --check $private/reports-raw-shals/${bucket} && exit 42 || exit 13"
    ;;

    canning)
    docker run $docksafe --network none -v=$private/canned:$private/canned debian:jessie /bin/bash -c "mkdir -p $private/canned/${bucket} && chown $uidno $private/canned/${bucket}"
    volargs="${volargs} --volume=$private/reports-raw/${bucket}:$private/reports-raw/${bucket}:ro"
    volargs="${volargs} --volume=$private/canned/${bucket}:$private/canned/${bucket}:rw"
    set -- /usr/local/bin/canning.py --start "$isofrom" --end "$isotill" \
        --reports-raw-root $private/reports-raw \
        --canned-root $private/canned
    ;;

    autoclaving)
    docker run $docksafe --network none -v=$public/autoclaved:$public/autoclaved debian:jessie /bin/bash -c "mkdir -p $public/autoclaved/${bucket} && chown $uidno $public/autoclaved/${bucket}"
    volargs="${volargs} --volume=$private/bridge_db:$private/bridge_db:ro"
    volargs="${volargs} --volume=$private/canned/${bucket}:$private/canned/${bucket}:ro"
    volargs="${volargs} --volume=$public/autoclaved/${bucket}:$public/autoclaved/${bucket}:rw"
    set -- /usr/local/bin/autoclaving.py --start "$isofrom" --end "$isotill" \
        --canned-root $private/canned \
        --bridge-db $private/bridge_db/bridge_db.json \
        --autoclaved-root $public/autoclaved
    ;;

    meta_pg)
    docknet="" # no `--network none'
    volargs="${volargs} --env-file=/etc/af-worker/hkgmetadb.env"
    volargs="${volargs} --volume=$public/autoclaved/${bucket}:$public/autoclaved/${bucket}:ro"
    set -- /usr/local/bin/centrifugation.py --start "$isofrom" --end "$isotill" \
        --autoclaved-root /data/ooni/public/autoclaved \
        --postgres "host=hkgmetadb.infra.ooni.io user=shovel dbname=metadb"
    ;;

    # XXX the two raw s3 operations should now read from glacier and an extra stage writing to glacier should be added
    reports_raw_s3_ls)
    docknet="" # no `--network none'
    volargs="${volargs} --env-file=/etc/af-worker/s3read.env"
    volargs="${volargs} --volume=$private/reports-raw-s3-ls:$private/reports-raw-s3-ls:rw"
    set -- /usr/local/bin/aws_s3_ls.py --url "s3://ooni-private/reports-raw/yaml/$bucket" --s3-ls "$private/reports-raw-s3-ls/${bucket}.json.gz"
    ;;

    tar_reports_raw)
    # some `reports-raw` buckets were already removed from datacollector and have to be restored from `.tar.lz4` for compression
    docker run $docksafe --network none -v=$private/reports-raw:$private/reports-raw debian:jessie /bin/bash -c "mkdir -p $private/reports-raw/${bucket} && chown $uidno $private/reports-raw/${bucket}"
    volargs="${volargs} --volume=$private/reports-tgz:$private/reports-tgz:rw"
    volargs="${volargs} --volume=$private/reports-raw/${bucket}:$private/reports-raw/${bucket}:rw"
    volargs="${volargs} --volume=$private/canned/${bucket}:$private/canned/${bucket}:ro"
    set -- /usr/local/bin/tar_reports_raw.py --bucket "$bucket" --reports-tgz "$private/reports-tgz" --reports-raw "$private/reports-raw" --canned "$private/canned"
    ;;

    reports_raw_cleanup)
    volargs="${volargs} --volume=$private/reports-raw-s3-ls:$private/reports-raw-s3-ls:ro"
    volargs="${volargs} --volume=$private/canned/${bucket}:$private/canned/${bucket}:ro"
    volargs="${volargs} --volume=$private/reports-raw/${bucket}:$private/reports-raw/${bucket}:rw"
    set -- /usr/local/bin/cleanup_reports_raw.py --bucket "${bucket}" --reports-raw-root "$private/reports-raw" --canned-root "$private/canned" --s3-ls "$private/reports-raw-s3-ls/${bucket}.json.gz"
    ;;

    sanitised_s3_ls)
    docknet="" # no `--network none'
    volargs="${volargs} --env-file=/etc/af-worker/s3read.env"
    volargs="${volargs} --volume=$public/sanitised-s3-ls:$public/sanitised-s3-ls:rw"
    set -- /usr/local/bin/aws_s3_ls.py --url "s3://ooni-data/sanitised/$bucket" --s3-ls "$public/sanitised-s3-ls/${bucket}.json.gz"
    ;;

    sanitised_check)
    volargs="${volargs} --volume=$public/autoclaved/${bucket}:$public/autoclaved/${bucket}:ro"
    volargs="${volargs} --volume=$public/sanitised/${bucket}:$public/sanitised/${bucket}:ro"
    volargs="${volargs} --volume=$public/santoken:$public/santoken:rw"
    set -- /usr/local/bin/check_sanitised.py --bucket "${bucket}" --sanitised-root "$public/sanitised" --autoclaved-root "$public/autoclaved" --santoken "$public/santoken/${bucket}"
    ;;

    sanitised_cleanup)
    volargs="${volargs} --volume=$public/autoclaved/${bucket}:$public/autoclaved/${bucket}:ro"
    volargs="${volargs} --volume=$public/sanitised-s3-ls:$public/sanitised-s3-ls:ro"
    volargs="${volargs} --volume=$public/santoken:$public/santoken:ro"
    volargs="${volargs} --volume=$public/sanitised/${bucket}:$public/sanitised/${bucket}:rw"
    set -- /usr/local/bin/cleanup_sanitised.py --bucket "${bucket}" --sanitised-root "$public/sanitised" --autoclaved-root "$public/autoclaved" --santoken "$public/santoken/${bucket}" --s3-ls "$public/sanitised-s3-ls/${bucket}.json.gz"
    ;;

    reports_tgz_s3_sync)
    docknet="" # no `--network none'
    volargs="${volargs} --env-file=/etc/af-worker/s3_ceo_backup_creator.env"
    volargs="${volargs} --volume=$private/reports-tgz:$private/reports-tgz:ro"
    set -- aws s3 sync --size-only "$private/reports-tgz/" s3://ooni-private/archives-raw/yaml/ --exclude '*' --include "${bucket}.tar.gz" --include "${bucket}.index.json.gz"
    ;;

    canned_s3_sync)
    docknet="" # no `--network none'
    volargs="${volargs} --env-file=/etc/af-worker/s3_ceo_backup_creator.env"
    volargs="${volargs} --volume=$private/canned/${bucket}:$private/canned/${bucket}:ro"
    set -- aws s3 sync --size-only "$private/canned/${bucket}/" "s3://ooni-private/canned/${bucket}/"
    ;;

    autoclaved_tarlz4_s3_sync)
    docknet="" # no `--network none'
    volargs="${volargs} --env-file=/etc/af-worker/s3root.env"
    volargs="${volargs} --volume=$public/autoclaved/${bucket}:$public/autoclaved/${bucket}:ro"
    set -- aws s3 sync --size-only "$public/autoclaved/${bucket}/" "s3://ooni-data/autoclaved/jsonl.tar.lz4/${bucket}/"
    ;;

    autoclaved_jsonl_s3_sync)
    docknet="" # no `--network none'
    volargs="${volargs} --env-file=/etc/af-worker/s3root.env"
    volargs="${volargs} --volume=$public/autoclaved/${bucket}:$public/autoclaved/${bucket}:ro"
    # disk-backed /tmp may be preferred as the largest report ever is ~1.4Gb,
    # concurrent processing of alike reports may be affected. OTOH, the usual
    # size of scratchpad is ~64M and it probably should not be an issue.
    volargs="${volargs} --tmpfs /tmp:rw,noexec,nosuid,size=1536m"
    set -- /usr/local/bin/aws_s3_lz4cat_sync.py --src "$public/autoclaved/${bucket}" --s3-bucket "ooni-data" --s3-prefix "autoclaved/jsonl/${bucket}"
    ;;

    *)
    echo "$0: unknown script <$script>" 1>&2
    exit 1
    ;;
esac

# FIXME: hardcoded uid of the `benchmark` user at `datacollector`
set -o xtrace
exec docker run $docksafe --user "$uidno" $docknet ${volargs} openobservatory/pipeline-shovel:20180201-6c3f7cef "$@"
